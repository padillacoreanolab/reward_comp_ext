{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Spike Sorting Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this introductory example, you will see how to use the :code:`spikeinterface` to perform a full electrophysiology analysis.\n",
    "- We will first create some simulated data, and we will then perform some pre-processing, run a couple of spike sorting algorithms, inspect and validate the results, export to Phy, and compare spike sorters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import glob\n",
    "import warnings\n",
    "import imp\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the figure size\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 6), dpi=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spikeinterface module by itself import only the spikeinterface.core submodule\n",
    "which is not useful for end user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import one by one different submodules separately (preferred).\n",
    "There are 5 modules:\n",
    "\n",
    "- :code:`extractors` : file IO\n",
    "- :code:`toolkit` : processing toolkit for pre-, post-processing, validation, and automatic curation\n",
    "- :code:`sorters` : Python wrappers of spike sorters\n",
    "- :code:`comparison` : comparison of spike sorting output\n",
    "- :code:`widgets` : visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface as si  # import core only\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.preprocessing as sp\n",
    "\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.widgets as sw\n",
    "from spikeinterface.exporters import export_to_phy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probeinterface import get_probe\n",
    "from probeinterface.plotting import plot_probe, plot_probe_group\n",
    "from probeinterface import write_prb, read_prb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mountainsort5 as ms5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "from mountainsort5.util import create_cached_recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also import all submodules at once with this\n",
    "Â  this internally import core+extractors+toolkit+sorters+comparison+widgets+exporters\n",
    "\n",
    "This is useful for notebooks but this is a more heavy import because internally many more dependency\n",
    "are imported (scipy/sklearn/networkx/matplotlib/h5py...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.full as si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase size of plot in jupyter\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Loading in the Probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reading in the probe information into Spike interface and plotting the probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_object = read_prb(\"./linear_probe_with_large_spaces.prb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_object.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_object.get_global_contact_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_object.get_global_device_channel_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a dictionary of all the variables in the probe file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'probe_parameters' in locals():\n",
    "    probe_dict = defaultdict(dict)\n",
    "    for attribute in dir(probe_parameters):\n",
    "        # Removing built in attributes\n",
    "        if not attribute.startswith(\"__\"): \n",
    "            probe_dict[attribute] = getattr(probe_parameters, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"probe_dict\" in locals():\n",
    "    for key, value in probe_dict.items():\n",
    "        print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Electrophysiology Recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are inputting the electrophsiology recording data with probe information. This should have been created in the prevous notebook in a directory created by Spike Interface. If you had already read in your own electrophsiology recording data with probe information with a different way, then follow these instructions.\n",
    "    - If you want to use a different directory, then you must either:\n",
    "        - Change `glob.glob({./path/to/with/*/recording_raw})` to the directory that you have the directories created from Spikeinterface. You can use a wildcard if you have multiple folders. You would replace `{./path/to/with/*/recording_raw}` with the path to either the parent directory or the actual directory containing the electrophsiology recording data read into Spikeinterface.\n",
    "        - Or change `(file_or_folder_or_dict={./path/to/recording_raw})`. You would replace `{./path/to/recording_raw}` with the path to either the parent directory or the actual directory containing the electrophsiology recording data read into Spikeinterface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_filepath_glob = \"/scratch/back_up/reward_competition_extention/data/rce_cohort_3/*/*.rec/*merged.rec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recording_files = glob.glob(recording_filepath_glob, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recording_files = [file_path for file_path in all_recording_files if \"copies\" not in file_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recording_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_files = [] \n",
    "failed_files = []\n",
    "\n",
    "for recording_file in all_recording_files:\n",
    "    print(recording_file)\n",
    "    \n",
    "    recording_basename = os.path.basename(recording_file)\n",
    "    recording_output_directory = \"/scratch/back_up/reward_competition_extention/proc/spike_sorting/{}\".format(recording_basename)\n",
    "    os.makedirs(recording_output_directory, exist_ok=True)\n",
    "\n",
    "    print(\"Output directory: {}\".format(recording_output_directory))\n",
    "    \n",
    "    try:\n",
    "        with open('successful_files.txt', \"r\") as myfile:\n",
    "            if recording_basename in myfile.read():\n",
    "                warnings.warn(\"\"\"Directory already exists for: {}.\n",
    "                              Either continue on if you are satisfied with the previous run \n",
    "                              or delete the directory and run this cell again\"\"\".format(recording_basename))\n",
    "                continue\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        trodes_recording = se.read_spikegadgets(recording_file, stream_id=\"trodes\")       \n",
    "        trodes_recording = trodes_recording.set_probes(probe_object)\n",
    "\n",
    "        child_spikesorting_output_directory = os.path.join(recording_output_directory,\"ss_output\")\n",
    "               \n",
    "        start = time.time()\n",
    "        # Make sure the recording is preprocessed appropriately\n",
    "        # lazy preprocessing\n",
    "        print(\"Running bandpass filter\")\n",
    "        recording_filtered = sp.bandpass_filter(trodes_recording, freq_min=300, freq_max=6000, dtype=np.float32)\n",
    "\n",
    "        print(\"Running whitening\")\n",
    "        recording_preprocessed: si.BaseRecording = sp.whiten(recording_filtered, dtype=np.float32)\n",
    "        \n",
    "        with TemporaryDirectory() as tmpdir:\n",
    "            # cache the recording to a temporary directory for efficient reading\n",
    "            print(\"Caching the recording\")\n",
    "            recording_cached = create_cached_recording(recording_preprocessed, folder=tmpdir)\n",
    "            recording_cached = recording_cached.set_probes(probe_object)\n",
    "            recording_cached.annotate(is_filtered=True)\n",
    "\n",
    "            print(\"Spike sorting\")\n",
    "            spike_sorted_object = ms5.sorting_scheme2(\n",
    "                recording=recording_cached,\n",
    "                sorting_parameters=ms5.Scheme2SortingParameters(\n",
    "                    detect_sign=0,\n",
    "                    phase1_detect_channel_radius=700,\n",
    "                    detect_channel_radius=700,\n",
    "                # other parameters...\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            assert isinstance(spike_sorted_object, si.BaseSorting)\n",
    "            \n",
    "            shutil.rmtree(child_spikesorting_output_directory, ignore_errors=True)\n",
    "            spike_sorted_object.save(folder=child_spikesorting_output_directory)\n",
    "\n",
    "            sw.plot_rasters(spike_sorted_object)\n",
    "            plt.title(recording_basename)\n",
    "            plt.ylabel(\"Unit IDs\")\n",
    "            \n",
    "            plt.savefig(os.path.join(recording_output_directory, \"{}_raster_plot.png\".format(recording_basename)))\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"Exporting waveforms\")\n",
    "            waveform_output_directory = os.path.join(recording_output_directory, \"waveforms\")\n",
    "            we_spike_sorted = si.extract_waveforms(\n",
    "                recording=recording_cached, \n",
    "                sorting=spike_sorted_object, \n",
    "                folder=waveform_output_directory,\n",
    "                ms_before=1, \n",
    "                ms_after=1, \n",
    "                progress_bar=True,\n",
    "                n_jobs=-1, \n",
    "                total_memory=\"8G\", \n",
    "                overwrite=True,\n",
    "                max_spikes_per_unit=2000,\n",
    "                sparse=False)\n",
    "            \n",
    "            print(\"we_spike_sorted is sparse: {}\".format(we_spike_sorted.is_sparse()))\n",
    "            \n",
    "            print(\"Saving to phy\")\n",
    "            phy_output_directory = os.path.join(recording_output_directory, \"phy\")\n",
    "            export_to_phy(we_spike_sorted, \n",
    "                            phy_output_directory,\n",
    "                            compute_pc_features=True, \n",
    "                            compute_amplitudes=True, \n",
    "                            remove_if_exists=True)\n",
    "            \n",
    "            # edit the params.py file os that it contains the correct realtive path\n",
    "            params_path = os.path.join(phy_output_directory, \"params.py\")\n",
    "            with open(params_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "            lines[0] = \"dat_path = r'./recording.dat'\\n\"\n",
    "            with open(params_path, 'w') as file:\n",
    "                file.writelines(lines)\n",
    "            \n",
    "            successful_files.append(recording_file)\n",
    "            with open('successful_files.txt', \"a+\") as fd:\n",
    "                fd.write(f'\\n{recording_basename}')                     \n",
    "\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        failed_files.append(recording_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
