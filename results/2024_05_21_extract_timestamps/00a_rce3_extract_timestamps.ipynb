{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3a02adc9e884466bc8c79db549cc3d2",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "bold": true,
       "underline": true
      },
      "toCodePoint": 17,
      "type": "marks"
     }
    ]
   },
   "source": [
    "# Time Stamp Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4546bee655b14a5dbf393161f1228e60",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Brief 1-2 sentence description of notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Supplement the description\n",
    "- Notebook that extracts the timestamps and gets the time that tones played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of all used packages and libraries\n",
    "import sys\n",
    "import os\n",
    "import git\n",
    "import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo = git.Repo(\".\", search_parent_directories=True)\n",
    "git_root = git_repo.git.rev_parse(\"--show-toplevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nancy/user/riwata/projects/reward_comp_ext'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(git_root, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of all used packages and libraries\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.helper\n",
    "import trodes.read_exported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_floats(s):\n",
    "    \"\"\"\n",
    "    Extracts all floats from a string and returns them as a list of strings.\n",
    "\n",
    "    Parameters:\n",
    "    - s (str): The string to extract floats from.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of strings, each representing a float found in the input string.\n",
    "    \"\"\"\n",
    "    float_pattern = r\"[-+]?\\d*\\.\\d+|\\d+\"\n",
    "    return [str(float(num)) for num in re.findall(float_pattern, s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d290bac2c17940bfbc0f9296beaf70e5",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Inputs & Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e528ce19c608425292151930d380f49f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "- Explanation of each input and where it comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs and Required data loading\n",
    "- input variable names are in all caps snake case\n",
    "- Whenever an input changes or is used for processing \n",
    "- The variables are all lower in snake case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "6cf83a5811054461a718a71673d09aab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 373,
    "execution_start": 1691424003628,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Path of the directory that contains the Spike Gadgets recording and the exported timestamp files\n",
    "# Exported with this tool https://docs.spikegadgets.com/en/latest/basic/ExportFunctions.html\n",
    "# Export these files:\n",
    "    # -raw – Continuous raw band export.\n",
    "    # -dio – Digital IO channel state change export.\n",
    "    # -analogio – Continuous analog IO export.\n",
    "INPUT_DIR = \"/scratch/back_up/reward_competition_extention/data/rce_cohort_3\"\n",
    "OUTPUT_DIR = r\"./proc\" # where data is saved should always be shown in the inputs\n",
    "TONE_DIN = \"dio_ECU_Din1\"\n",
    "TONE_STATE = 1\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_PREFIX = \"rce_pilot_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_KEEP = ['session_dir', 'recording', 'metadata_dir', 'metadata_file',\n",
    "'original_file', 'filename', 'session_path', 'all_subjects',\n",
    "       'current_subject', 'event_timestamps', 'video_name',\n",
    "       'video_timestamps', 'event_frames', 'first_item_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_COLS_TO_KEEP = ['session_dir',\n",
    " 'recording',\n",
    " 'original_file',\n",
    " 'session_path',\n",
    " 'current_subject',\n",
    " 'first_item_data',\n",
    " 'first_timestamp',\n",
    " 'all_subjects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_COLS_TO_KEEP = ['session_dir',\n",
    " 'metadata_file',\n",
    " 'event_timestamps',\n",
    " 'video_name',\n",
    " 'video_timestamps',\n",
    " 'event_frames',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_columns = ['session_dir', 'video_name']\n",
    "different_columns = ['metadata_file', 'event_frames', 'event_timestamps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find way not to hard code this\n",
    "# ALL_SESSION_DIR = glob.glob(\"/scratch/back_up/reward_competition_extention/data/standard/2023_06_*/*.rec\")\n",
    "ALL_SESSION_DIR = glob.glob(\"/scratch/back_up/reward_competition_extention/data/rce_cohort_3/*/*.rec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/back_up/reward_competition_extention/data/rce_cohort_3/discriminate/20240417_150153_comp_discriminate_subj_3-3_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/discriminate/20240413_151204_comp_discriminate_subj_3-1_and_3-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/discriminate/20240414_154115_comp_discriminate_subj_4-2_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/discriminate/20240415_153312_comp_discriminate_subj_3-1_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/discriminate/20240415_165702_comp_discriminate_subj_4-3_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/discriminate/20240415_140712_comp_discriminate_subj_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/discriminate/20240413_163726_comp_discriminate_subj_4-2_and_4-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/long_comp/20240321_114851_long_comp_subj_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/long_comp/20240317_172017_long_comp_subj_4-2_and_4-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/long_comp/20240317_151922_long_comp_subj_3-1_and_3-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/long_comp/20240319_160457_long_comp_subj_4-2_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/long_comp/20240319_134914_long_comp_subj_3-1_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/long_comp/20240318_143819_long_comp_subj_3-3_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/long_comp/20240320_114629_long_comp_subj_5-3_and_5-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/long_comp/20240318_170933_long_comp_subj_4-3_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240411_155157_comp_novel_subj_3-1_and_3-4_and_4-2_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240412_161135_comp_novel_subj_4-2_and_4-4_and_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240406_150017_comp_novel_subj_4-3_and_4-4_and_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240408_155308_comp_novel_subj_3-1_and_3-4_and_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240401_151442_comp_novel_subj_3-1_and_3-3_and_4-2_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240402_145808_comp_novel_subj_3-3_and_3-4_and_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240407_152222_comp_novel_subj_3-1_and_3-3_and_4-2_and_4-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240410_152009_comp_novel_subj_4-2_and_4-3_and_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240405_162313_comp_novel_subj_3-1_and_3-4_and_4-2_and_4-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/novel/20240409_142051_comp_novel_subj_3-3_and_3-4_and_4-3_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/alone_comp/20240323_144517_alone_comp_subj_3-1_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/alone_comp/20240323_122227_alone_comp_subj_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/alone_comp/20240320_142408_alone_comp_subj_3-1_and_3-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/alone_comp/20240322_160946_alone_comp_subj_4-3_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/alone_comp/20240322_120625_alone_comp_subj_3-3_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/alone_comp/20240320_171038_alone_comp_subj_4-2_and_4-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/alone_comp/20240323_165815_alone_comp_subj_4-2_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/omission/20240325_125522_comp_om_subj_3-1_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/omission/20240318_115841_comp_om_subj_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/omission/20240317_105929_comp_om_subj_5-3_and_5-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/omission/20240227_130241_comp_om_subj_4-2_and_4-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/omission/20240319_111204_comp_om_subj_5-2_and_5-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/omission/20240325_150329_comp_om_subj_4-2_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/omission/20240229_152936_comp_om_subj_3-3_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/omission/20240228_142038_comp_om_subj_3-1_and_3-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/omission/20240228_154053_comp_om_subj_4-3_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/both_rewarded/20240330_141834_comp_both_subj_3-3_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/both_rewarded/20240329_160356_comp_both_subj_4_2_and_4-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/both_rewarded/20240327_165151_comp_both_subj_4-2_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/both_rewarded/20240331_164629_comp_both_subj_4-3_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/both_rewarded/20240331_152930_comp_both_subj_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/both_rewarded/20240327_153306_comp_both_subj_3-1_and_3-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/both_rewarded/20240329_144111_comp_both_subj_3-1_and_3-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/both_rewarded/20240327_141822_comp_both_subj_5-2_and_5-3.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/both_rewarded/20240330_153409_comp_both_subj_4-3_and_4-4.rec',\n",
       " '/scratch/back_up/reward_competition_extention/data/rce_cohort_3/2024_5_9_rce3_hco.rec/2024_5_9_rce3_hco.rec']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_SESSION_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e3ee4891d43a4ac287413afc552ca289",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9ccbf6cc70fd4d379fa29317f733771f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe each output that the notebook creates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fc8e8920a6944918a15fac575cdf6e78",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- Is it a plot or is it data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1e639d4776a84aa9ac8ded2e14fa57db",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- How valuable is the output and why is it valuable or useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw directory\n",
    "- raw_group0.dat\n",
    "    - voltage_value: Array with voltage measurement for each channel at each timestamp\n",
    "- timestamps.dat\n",
    "    - voltage_time_stamp: The time stamp of each voltage measurement\n",
    "\n",
    "parent directory\n",
    "- 1.videoTimeStamps.cameraHWSync\n",
    "    - frame_number: Calculated by getting the index of each video time stamp tuple \n",
    "    - PosTimestamp: The time stamp of each video frame\n",
    "    - HWframeCount: Unknown value. Starts at 30742 and increases by 1 for each tuple  \n",
    "    - HWTimestamp: Unknown value. All zeroes\n",
    "    - video_time: Calculated by dividing the frame number by the fps(frames per second) \n",
    "    - video_seconds: video_time, but rounded to seconds  \t\n",
    "    - These are filled in versions of the above collumns with the value from the most recent previous cell\n",
    "        - filled_PosTimestamp \t\n",
    "        - filledHWframeCount \t\n",
    "        - filled_frame_number \t\n",
    "        - filled_video_time \t\n",
    "        - filled_video_seconds \t\n",
    "\n",
    "DIO directory\n",
    "- dio_ECU_Din1.dat\n",
    "    - time: The time stamp the corresponds to the DIN input\n",
    "    - state: Binary state of whether there is input from DIN or not \t\n",
    "    - trial_number: Calculated by adding 1 to every time there is a DIN input\n",
    "    - These are filled in versions of the above collumns with the value from the most recent previous cell\n",
    "        - filled_state \t\n",
    "        - filled_trial_number\n",
    "\n",
    "ss_output directory (Spike sorting with Spike interface)\n",
    "- firings.npz\n",
    "    - unit_id: All the units that had a spike train for the given timestamp \t\n",
    "    - number_of_units: Calculated by counting the number of units that had a spike train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- function names are short and in snake case all lowercase\n",
    "- a function name should be unique but does not have to describe the function\n",
    "- doc strings describe functions not function names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8999d19b6b7d4d63bc90f0b0bd9ab085",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9b36cdf08567463082b005cb0dec684b",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe what is done to the data here and how inputs are manipulated to generate outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "89aaba237c644628b1b37604b75e7cb1",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# As much code and as many cells as required\n",
    "# includes EDA and playing with data\n",
    "# GO HAM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOP 1: Extracting all the Trodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting all the data from all the exported Trodes files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting all the data from all the exported Trodes files and saving it to `session_to_trodes_data`\n",
    "    - Creates a dictionary with the structure of:\n",
    "        - `{dir_name: {file_name: metadata, file_name_2: metadata_2}, dir_name_2: {file_name_3: metadata_3, file_name_4: metadata_4}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Session: 20240417_150153_comp_discriminate_subj_3-3_and_3-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nancy/user/riwata/projects/reward_comp_ext/src/trodes/read_exported.py:62: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  return np.dtype(dtype_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file 20240417_150153_comp_discriminate_subj_3-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240417_150153_comp_discriminate_subj_3-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240413_151204_comp_discriminate_subj_3-1_and_3-3\n",
      "Skipping file 20240413_151204_comp_discriminate_subj_3-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240413_151204_comp_discriminate_subj_3-1_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240414_154115_comp_discriminate_subj_4-2_and_4-4\n",
      "Skipping file 20240414_154115_comp_discriminate_subj_4-4_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240414_154115_comp_discriminate_subj_4-2_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240415_153312_comp_discriminate_subj_3-1_and_3-4\n",
      "Skipping file 20240415_153312_comp_discriminate_subj_3-4_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240415_153312_comp_discriminate_subj_3-1_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240415_165702_comp_discriminate_subj_4-3_and_4-4\n",
      "Skipping file 20240415_165702_comp_discriminate_subj_4-4_t2b2_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240415_165702_comp_discriminate_subj_4-3_t1b1_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240415_140712_comp_discriminate_subj_5-2_and_5-3\n",
      "Skipping file 20240415_140712_comp_discriminate_subj_5-2_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240415_140712_comp_discriminate_subj_5-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240413_163726_comp_discriminate_subj_4-2_and_4-3\n",
      "Skipping file 20240413_163726_comp_discriminate_subj_4-3_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240413_163726_comp_discriminate_subj_4-2_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240321_114851_long_comp_subj_5-2_and_5-3\n",
      "Skipping file 20240321_114851_long_comp_subj_5-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240321_114851_long_comp_subj_5-2_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240317_172017_long_comp_subj_4-2_and_4-3\n",
      "Skipping file logger_raw.dat due to error: 'ascii' codec can't decode byte 0xa0 in position 14: ordinal not in range(128)\n",
      "Skipping file logger_raw.dat due to error: 'ascii' codec can't decode byte 0xe0 in position 18: ordinal not in range(128)\n",
      "Skipping file 20240317_172017_long_comp_subj_4-2_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240317_172017_long_comp_subj_4-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240317_151922_long_comp_subj_3-1_and_3-3\n",
      "Skipping file 20240317_151922_long_comp_subj_3-1_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240317_151922_long_comp_subj_3-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240319_160457_long_comp_subj_4-2_and_4-4\n",
      "Skipping file 20240319_160457_long_comp_subj_4-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240319_160457_long_comp_subj_4-2_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240319_134914_long_comp_subj_3-1_and_3-4\n",
      "Skipping file 20240319_134914_long_comp_subj_3-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240319_134914_long_comp_subj_3-1_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240318_143819_long_comp_subj_3-3_and_3-4\n",
      "Skipping file 20240318_143819_long_comp_subj_3-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240318_143819_long_comp_subj_3-4_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240320_114629_long_comp_subj_5-3_and_5-4\n",
      "Skipping file 20240320_114629_long_comp_subj_5-4_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240320_114629_long_comp_subj_5-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240318_170933_long_comp_subj_4-3_and_4-4\n",
      "Skipping file 20240318_170933_long_comp_subj_4-4_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240318_170933_long_comp_subj_4-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240411_155157_comp_novel_subj_3-1_and_3-4_and_4-2_and_4-4\n",
      "Skipping file 20240411_155157_comp_novel_subj_3-1_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240411_155157_comp_novel_subj_4-2_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240411_155157_comp_novel_subj_3-4_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240412_161135_comp_novel_subj_4-2_and_4-4_and_5-2_and_5-3\n",
      "Skipping file 20240412_161135_comp_novel_subj_5-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240412_161135_comp_novel_subj_4-4_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240412_161135_comp_novel_subj_5-2_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240412_161135_comp_novel_subj_4-2_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240406_150017_comp_novel_subj_4-3_and_4-4_and_5-2_and_5-3\n",
      "Skipping file 20240406_150017_comp_novel_subj_4-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240406_150017_comp_novel_subj_4-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240406_150017_comp_novel_subj_5-2_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240406_150017_comp_novel_subj_5-3_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240408_155308_comp_novel_subj_3-1_and_3-4_and_5-2_and_5-3\n",
      "Skipping file 20240408_155308_comp_novel_subj_3-1_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240408_155308_comp_novel_subj_5-2_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240408_155308_comp_novel_subj_3-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240408_155308_comp_novel_subj_5-3_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240401_151442_comp_novel_subj_3-1_and_3-3_and_4-2_and_4-4\n",
      "Skipping file 20240401_151442_comp_novel_subj_4-4_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240401_151442_comp_novel_subj_3-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240401_151442_comp_novel_subj_4-2_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240401_151442_comp_novel_subj_3-1_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240402_145808_comp_novel_subj_3-3_and_3-4_and_5-2_and_5-3\n",
      "Skipping file 20240402_145808_comp_novel_subj_5-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240402_145808_comp_novel_subj_5-2_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240402_145808_comp_novel_subj_3-4_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240402_145808_comp_novel_subj_3-3_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240407_152222_comp_novel_subj_3-1_and_3-3_and_4-2_and_4-3\n",
      "Skipping file 20240407_152222_comp_novel_subj_4-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240407_152222_comp_novel_subj_3-3_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240407_152222_comp_novel_subj_4-2_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240407_152222_comp_novel_subj_3-1_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240410_152009_comp_novel_subj_4-2_and_4-3_and_5-2_and_5-3\n",
      "Skipping file 20240410_152009_comp_novel_subj_5-2_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240410_152009_comp_novel_subj_4-2_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240410_152009_comp_novel_subj_5-3_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240410_152009_comp_novel_subj_4-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240405_162313_comp_novel_subj_3-1_and_3-4_and_4-2_and_4-3\n",
      "Skipping file 20240405_162313_comp_novel_subj_4-3_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240405_162313_comp_novel_subj_3-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240405_162313_comp_novel_subj_3-1_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240409_142051_comp_novel_subj_3-3_and_3-4_and_4-3_and_4-4\n",
      "Skipping file 20240409_142051_comp_novel_subj_3-4_t4b4_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240409_142051_comp_novel_subj_3-3_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240409_142051_comp_novel_subj_4-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240409_142051_comp_novel_subj_4-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240323_144517_alone_comp_subj_3-1_and_3-4\n",
      "Skipping file 20240323_144517_alone_comp_subj_3-1_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240323_144517_alone_comp_subj_3-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240323_122227_alone_comp_subj_5-2_and_5-3\n",
      "Skipping file 20240323_122227_alone_comp_subj_5-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240323_122227_alone_comp_subj_5-2_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240320_142408_alone_comp_subj_3-1_and_3-3\n",
      "Skipping file 20240320_142408_alone_comp_subj_3-1_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240320_142408_alone_comp_subj_3-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240322_160946_alone_comp_subj_4-3_and_4-4\n",
      "Skipping file 20240322_160946_alone_comp_subj_4-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240322_160946_alone_comp_subj_4-4_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240322_120625_alone_comp_subj_3-3_and_3-4\n",
      "Skipping file 20240322_120625_alone_comp_subj_3-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240322_120625_alone_comp_subj_3-4_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240320_171038_alone_comp_subj_4-2_and_4-3\n",
      "Skipping file 20240320_171038_alone_comp_subj_4-2_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240320_171038_alone_comp_subj_4-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240323_165815_alone_comp_subj_4-2_and_4-4\n",
      "Skipping file 20240323_165815_alone_comp_subj_4-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240323_165815_alone_comp_subj_4-2_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240325_125522_comp_om_subj_3-1_and_3-4\n",
      "Skipping file 20240325_125522_comp_om_subj_3-4_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240325_125522_comp_om_subj_3-1_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240318_115841_comp_om_subj_5-2_and_5-3\n",
      "Skipping file 20240318_115841_comp_om_subj_5-3_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240318_115841_comp_om_subj_5-2_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240317_105929_comp_om_subj_5-3_and_5-4\n",
      "Skipping file 20240317_105929_comp_om_subj_5-4_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240317_105929_comp_om_subj_5-3_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240227_130241_comp_om_subj_4-2_and_4-3\n",
      "Skipping file 20240227_130241_comp_om_subj_4-2_t1b1_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240227_130241_comp_om_subj_4-3_t2b2_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240319_111204_comp_om_subj_5-2_and_5-4\n",
      "Skipping file 20240319_111204_comp_om_subj_5-4_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240319_111204_comp_om_subj_5-2_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240325_150329_comp_om_subj_4-2_and_4-4\n",
      "Skipping file 20240325_150329_comp_om_subj_4-2_t5b5_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240325_150329_comp_om_subj_4-4_t6b6_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Current Session: 20240229_152936_comp_om_subj_3-3_and_3-4\n",
      "Skipping file 20240229_152936_comp_om_subj_3-4_t1b1_merged.timestampoffset.txt due to error: Settings format not supported\n",
      "Skipping file 20240229_152936_comp_om_subj_3-3_t3b3_merged.timestampoffset.txt due to error: Settings format not supported\n"
     ]
    }
   ],
   "source": [
    "# Saving the trodes data for each session\n",
    "# Each key is a session name\n",
    "# Each value is a dictionary of every recording file in that session\n",
    "session_to_trodes_data = utilities.helper.create_recursive_dict()\n",
    "\n",
    "\n",
    "# Saving the path of the session recording\n",
    "session_to_path = {}\n",
    "\n",
    "# Going through each session recording\n",
    "# Which includes all the recordings from all the miniloggers and cameras\n",
    "for session_path in ALL_SESSION_DIR:   \n",
    "    try:\n",
    "        # Getting the name of the session from the path\n",
    "        session_basename = os.path.splitext(os.path.basename(session_path))[0]\n",
    "        print(\"Current Session: {}\".format(session_basename))\n",
    "        # Reading the trodes data for every recording file in the session directory\n",
    "        session_to_trodes_data[session_basename] = trodes.read_exported.organize_all_trodes_export(session_path)\n",
    "        \n",
    "        session_to_path[session_basename] = session_path\n",
    "    except Exception as e: \n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_to_trodes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding the video timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session_path in ALL_SESSION_DIR:   \n",
    "    try:\n",
    "        session_basename = os.path.splitext(os.path.basename(session_path))[0]\n",
    "        print(\"Current Session: {}\".format(session_basename))\n",
    "        file_to_video_timestamps = {}\n",
    "        for video_timestamps in glob.glob(os.path.join(session_path, \"*cameraHWSync\")):\n",
    "            video_basename = os.path.basename(video_timestamps)\n",
    "            print(\"Current Video Name: {}\".format(video_basename))\n",
    "            timestamp_array = trodes.read_exported.read_trodes_extracted_data_file(video_timestamps)\n",
    "            if \"video_timestamps\" not in session_to_trodes_data[session_basename][session_basename]:\n",
    "                session_to_trodes_data[session_basename][session_basename][\"video_timestamps\"] = defaultdict(dict)\n",
    "            session_to_trodes_data[session_basename][session_basename][\"video_timestamps\"][video_basename.split(\".\")[-3]] = timestamp_array\n",
    "    \n",
    "    \n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_to_trodes_data[session_basename][session_basename][\"video_timestamps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a dataframe the dictionary with a column for:\n",
    "  - Session directory\n",
    "  - Recording name\n",
    "  - Metadata directory\n",
    "  - Metadata file\n",
    "  - And a column for each metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the nested dictionary\n",
    "trodes_metadata_df = pd.DataFrame.from_dict({(i,j,k,l): session_to_trodes_data[i][j][k][l] \n",
    "                           for i in session_to_trodes_data.keys() \n",
    "                           for j in session_to_trodes_data[i].keys()\n",
    "                           for k in session_to_trodes_data[i][j].keys()\n",
    "                           for l in session_to_trodes_data[i][j][k].keys()},\n",
    "                           orient='index')\n",
    "\n",
    "# Resetting the index and renaming the columns\n",
    "trodes_metadata_df = trodes_metadata_df.reset_index()\n",
    "trodes_metadata_df = trodes_metadata_df.rename(columns={'level_0': 'session_dir', 'level_1': 'recording', 'level_2': 'metadata_dir', 'level_3': 'metadata_file'}, errors=\"ignore\")\n",
    "\n",
    "# Adding the session path to the dataframe\n",
    "trodes_metadata_df[\"session_path\"] = trodes_metadata_df[\"session_dir\"].map(session_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the first item from each tuple in the arrays in the `data` column\n",
    "  - This first item is usually just the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"data\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dtype name of each column in the numpy array\n",
    "trodes_metadata_df[\"first_dtype_name\"] = trodes_metadata_df[\"data\"].apply(lambda x: x.dtype.names[0])\n",
    "# Getting the first item of each column in the numpy array\n",
    "trodes_metadata_df[\"first_item_data\"] = trodes_metadata_df[\"data\"].apply(lambda x: x[x.dtype.names[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but for the last column\n",
    "trodes_metadata_df[\"last_dtype_name\"] = trodes_metadata_df[\"data\"].apply(lambda x: x.dtype.names[-1])\n",
    "trodes_metadata_df[\"last_item_data\"] = trodes_metadata_df[\"data\"].apply(lambda x: x[x.dtype.names[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"recording\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the subject information from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_multiple_delimiters(s, delimiters):\n",
    "    \"\"\"\n",
    "    Splits a string by multiple delimiters.\n",
    "\n",
    "    Parameters:\n",
    "    - s (str): The string to split.\n",
    "    - delimiters (list): A list of delimiters to split the string by.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of substrings.\n",
    "    \"\"\"\n",
    "    return re.split('|'.join(map(re.escape, delimiters)), s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"all_subjects\"] = trodes_metadata_df[\"session_dir\"].apply(lambda x: x.split(\"subj\")[-1].strip(\"_\").replace(\"-\", \".\"))#.split(\"t\")[0].strip(\"_\").replace(\"_\",\".\").split(\".and.\"))\n",
    "trodes_metadata_df[\"all_subjects\"] = trodes_metadata_df[\"all_subjects\"].apply(lambda x: sorted(extract_floats(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"session_dir\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"all_subjects\"].apply(lambda x: tuple(x)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"current_subject\"] = trodes_metadata_df[\"recording\"].apply(lambda x: x.split(\"subj\")[-1].strip(\"_\").replace(\"-\", \".\").replace(\"_\", \".\"))#.split(\"t\")[0].strip(\"_\").replace(\"_\",\".\").split(\".and.\"))\n",
    "trodes_metadata_df[\"current_subject\"] = trodes_metadata_df[\"current_subject\"].apply(lambda x: str(extract_floats(x)[0]).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"current_subject\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping all the rows with unneeded metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"metadata_dir\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_TO_KEEP = ['raw', 'DIO', 'video_timestamps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df = trodes_metadata_df[trodes_metadata_df[\"metadata_dir\"].isin(METADATA_TO_KEEP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df = trodes_metadata_df[~trodes_metadata_df[\"metadata_file\"].str.contains(\"out\")]\n",
    "trodes_metadata_df = trodes_metadata_df[~trodes_metadata_df[\"metadata_file\"].str.contains(\"coordinates\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df = trodes_metadata_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the first time stamp of each recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_raw_df = trodes_metadata_df[(trodes_metadata_df[\"metadata_dir\"] == \"raw\") & (trodes_metadata_df[\"metadata_file\"] == \"timestamps\")].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_raw_df[\"first_timestamp\"] = trodes_raw_df[\"first_item_data\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_raw_df[\"recording\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_to_first_timestamp = trodes_raw_df.set_index('session_dir')['first_timestamp'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_to_first_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"first_timestamp\"] = trodes_metadata_df[\"session_dir\"].map(recording_to_first_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df[\"first_timestamp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the event timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_metadata_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trodes_state_df = trodes_metadata_df[trodes_metadata_df[\"last_dtype_name\"] == \"state\"].copy()\n",
    "\n",
    "# Filtering for digital IO channels\n",
    "trodes_state_df = trodes_metadata_df[trodes_metadata_df[\"metadata_dir\"].isin([\"DIO\"])].copy()\n",
    "# Filtering for tone and port entry related channels\n",
    "trodes_state_df = trodes_metadata_df[trodes_metadata_df[\"id\"].isin([\"ECU_Din1\", \"ECU_Din2\", \"ECU_Din3\"])].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df[\"event_indexes\"] = trodes_state_df.apply(lambda x: np.column_stack([np.where(x[\"last_item_data\"] == 1)[0], np.where(x[\"last_item_data\"] == 1)[0]+1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df[\"event_indexes\"] = trodes_state_df.apply(lambda x: x[\"event_indexes\"][x[\"event_indexes\"][:, 1] <= x[\"first_item_data\"].shape[0] - 1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df[\"event_timestamps\"] = trodes_state_df.apply(lambda x: x[\"first_item_data\"][x[\"event_indexes\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the video timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syncing up the video frame data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the rows that are the metadata for the video timestamps\n",
    "trodes_video_df = trodes_metadata_df[trodes_metadata_df[\"metadata_dir\"] == \"video_timestamps\"].copy().reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for the first video only\n",
    "# This only applies to this pilot data where we are only looking the at competition data\n",
    "# trodes_video_df = trodes_video_df[trodes_video_df[\"metadata_file\"] == \"1\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_video_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the video timestamps into an evenly distributed array\n",
    "trodes_video_df[\"video_timestamps\"] = trodes_video_df[\"first_item_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the columns that are no longer needed\n",
    "trodes_video_df = trodes_video_df[[\"filename\", \"video_timestamps\", \"session_dir\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the filename so that we can merge with other dataframes with the same column name\n",
    "trodes_video_df = trodes_video_df.rename(columns={\"filename\": \"video_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_video_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding each video as a row to each state row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df = pd.merge(trodes_state_df, trodes_video_df, on=[\"session_dir\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the closest frame to each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df[\"event_timestamps\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df[\"event_frames\"] = trodes_state_df.apply(lambda x: utilities.helper.find_nearest_indices(x[\"event_timestamps\"], x[\"video_timestamps\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine raw and state dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df = trodes_state_df[STATE_COLS_TO_KEEP].drop_duplicates(subset=[\"session_dir\", \"video_name\", \"metadata_file\"]).sort_values([\"session_dir\", \"video_name\", \"metadata_file\"]).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df = trodes_state_df.groupby(same_columns).agg({**{col: 'first' for col in trodes_state_df.columns if col not in same_columns + different_columns}, **{col: lambda x: x.tolist() for col in different_columns}}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df[\"tone_timestamps\"] = trodes_state_df[\"event_timestamps\"].apply(lambda x: x[0])\n",
    "trodes_state_df[\"box_1_port_entry_timestamps\"] = trodes_state_df[\"event_timestamps\"].apply(lambda x: x[1])\n",
    "trodes_state_df[\"box_2_port_entry_timestamps\"] = trodes_state_df[\"event_timestamps\"].apply(lambda x: x[2])\n",
    "\n",
    "trodes_state_df[\"tone_frames\"] = trodes_state_df[\"event_frames\"].apply(lambda x: x[0])\n",
    "trodes_state_df[\"box_1_port_entry_frames\"] = trodes_state_df[\"event_frames\"].apply(lambda x: x[1])\n",
    "trodes_state_df[\"box_2_port_entry_frames\"] = trodes_state_df[\"event_frames\"].apply(lambda x: x[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df = trodes_state_df.drop(columns=[\"event_timestamps\", \"event_frames\", \"metadata_file\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_raw_df = trodes_raw_df[RAW_COLS_TO_KEEP].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df = pd.merge(trodes_raw_df, trodes_state_df, on=[\"session_dir\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df = trodes_final_df.rename(columns={\"first_item_data\": \"raw_timestamps\"})\n",
    "trodes_final_df = trodes_final_df.drop(columns=[\"metadata_file\"], errors=\"ignore\")\n",
    "trodes_final_df = trodes_final_df.sort_values([\"session_dir\", \"recording\"]).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the timestamps 0 indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df[[col for col in trodes_final_df.columns if \"timestamps\" in col]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df[\"last_timestamp\"] = trodes_final_df[\"raw_timestamps\"].apply(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropping raw timestamps because of memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df = trodes_final_df.drop(columns=[\"raw_timestamps\", \"original_file\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_trodes_final_df = trodes_final_df.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [col for col in trodes_final_df.columns if \"timestamps\" in col]:\n",
    "    trodes_final_df[col] = trodes_final_df.apply(lambda x: x[col].astype(np.int32) - np.int32(x[\"first_timestamp\"]), axis=1)\n",
    "\n",
    "for col in [col for col in trodes_final_df.columns if \"frames\" in col]:\n",
    "    trodes_final_df[col] = trodes_final_df[col].apply(lambda x: x.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_columns = sorted(trodes_final_df.columns\n",
    ", key=lambda x: x.split(\"_\")[-1])\n",
    "trodes_final_df = trodes_final_df[sorted_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df.to_pickle(os.path.join(OUTPUT_DIR, \"{}_00_trodes_metadata.pkl\".format(OUTPUT_PREFIX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df[\"session_dir\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trodes_final_df[\"video_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cf8fe3695d074ee7887fdf6459cbf5ce",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
